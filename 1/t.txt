import numpy as np
import random
import pickle
from collections import defaultdict

class TicTacToe:
    def __init__(self):
        self.reset()

    def reset(self):
        self.board = [' '] * 9
        self.current_winner = None
        return self.get_state()

    def get_state(self):
        return ''.join(self.board)

    def available_actions(self):
        return [i for i, spot in enumerate(self.board) if spot == ' ']

    def make_move(self, square, letter):
        if self.board[square] == ' ':
            self.board[square] = letter
            if self.winner(square, letter):
                self.current_winner = letter
            return True
        return False

    def winner(self, square, letter):
        row_ind = square // 3
        row = self.board[row_ind*3:(row_ind+1)*3]
        if all([s == letter for s in row]):
            return True
        col_ind = square % 3
        col = [self.board[col_ind+i*3] for i in range(3)]
        if all([s == letter for s in col]):
            return True
        diag1 = [self.board[i] for i in [0, 4, 8]]
        diag2 = [self.board[i] for i in [2, 4, 6]]
        if all([s == letter for s in diag1]) or all([s == letter for s in diag2]):
            return True
        return False

    def is_draw(self):
        return ' ' not in self.board

class QLearningAgent:
    def __init__(self, alpha=0.1, gamma=0.95, epsilon=1.0, epsilon_decay=0.9995, epsilon_min=0.01):
        self.q_table = defaultdict(float)
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

    def get_q(self, state, action):
        return self.q_table[(state, action)]

    def choose_action(self, state, available_actions):
        if random.random() < self.epsilon:
            return random.choice(available_actions)
        qs = [self.get_q(state, a) for a in available_actions]
        max_q = max(qs)
        return random.choice([a for a, q in zip(available_actions, qs) if q == max_q])

    def learn(self, s, a, r, s_, available_actions):
        max_future_q = max([self.get_q(s_, a_) for a_ in available_actions]) if available_actions else 0
        old_q = self.q_table[(s, a)]
        self.q_table[(s, a)] = old_q + self.alpha * (r + self.gamma * max_future_q - old_q)
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def save(self, filename='q_table.pkl'):
        with open(filename, 'wb') as f:
            pickle.dump(dict(self.q_table), f)

    def load(self, filename='q_table.pkl'):
        with open(filename, 'rb') as f:
            loaded_q = pickle.load(f)
            self.q_table.update(loaded_q)


def train_agent(episodes=50000):
    env = TicTacToe()
    agent = QLearningAgent()
    results = {'win': 0, 'loss': 0, 'draw': 0}

    for episode in range(episodes):
        state = env.reset()

        while True:
            available_actions = env.available_actions()
            action = agent.choose_action(state, available_actions)
            env.make_move(action, 'X')
            new_state = env.get_state()

            if env.current_winner == 'X':
                agent.learn(state, action, 1, new_state, [])
                results['win'] += 1
                break
            elif env.is_draw():
                agent.learn(state, action, 0.5, new_state, [])
                results['draw'] += 1
                break

            opp_action = random.choice(env.available_actions())
            env.make_move(opp_action, 'O')

            if env.current_winner == 'O':
                agent.learn(state, action, -1, new_state, [])
                results['loss'] += 1
                break
            elif env.is_draw():
                agent.learn(state, action, 0.5, new_state, [])
                results['draw'] += 1
                break
            else:
                next_state = env.get_state()
                agent.learn(state, action, 0, next_state, env.available_actions())
                state = next_state

        if episode % 5000 == 0:
            print(f"Episode {episode} - Epsilon: {agent.epsilon:.4f}")

    print("âœ… Training complete!")
    print(f"Wins: {results['win']} | Losses: {results['loss']} | Draws: {results['draw']}")
    agent.save()
    return agent


def print_board(board):
    print('\n'.join([' | '.join(board[i*3:(i+1)*3]) for i in range(3)]))
    print()

def play_against_agent(agent):
    env = TicTacToe()
    state = env.reset()
    print("Welcome to Tic-Tac-Toe!")
    print("You are O. Agent is X.")
    print("Board positions:\n")
    print("0 | 1 | 2\n3 | 4 | 5\n6 | 7 | 8\n")

    while True:
        print_board(env.board)

        move = -1
        while move not in env.available_actions():
            try:
                move = int(input("Enter your move (0-8): "))
            except:
                continue

        env.make_move(move, 'O')
        if env.current_winner == 'O':
            print_board(env.board)
            print("ðŸŽ‰ You win!")
            break
        elif env.is_draw():
            print_board(env.board)
            print("ðŸ¤ It's a draw!")
            break

        state = env.get_state()
        action = agent.choose_action(state, env.available_actions())
        env.make_move(action, 'X')
        print(f"\nðŸ¤– Agent plays: {action}")
        if env.current_winner == 'X':
            print_board(env.board)
            print("ðŸ’€ Agent wins!")
            break
        elif env.is_draw():
            print_board(env.board)
            print("ðŸ¤ It's a draw!")
            break

if __name__ == "__main__":
    agent = train_agent(episodes=50000)
    play_against_agent(agent)
