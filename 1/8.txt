import numpy as np

# Gridworld parameters
grid_size = 4
num_states = grid_size * grid_size
actions = ['up', 'down', 'left', 'right']
action_idx = {a: i for i, a in enumerate(actions)}
num_actions = len(actions)

# Rewards and terminal states
terminal_states = [0, num_states - 1]
reward_step = -1

def next_state(s, a):
    row, col = divmod(s, grid_size)
    if s in terminal_states:
        return s  # no moves from terminal states

    if a == 'up':
        row = max(row - 1, 0)
    elif a == 'down':
        row = min(row + 1, grid_size - 1)
    elif a == 'left':
        col = max(col - 1, 0)
    elif a == 'right':
        col = min(col + 1, grid_size - 1)
    return row * grid_size + col

# Create transition probabilities and rewards
# Here deterministic transitions (prob=1)
P = np.zeros((num_states, num_actions, num_states))
R = np.full((num_states, num_actions), reward_step)

for s in range(num_states):
    for a_i, a in enumerate(actions):
        s_prime = next_state(s, a)
        P[s, a_i, s_prime] = 1.0
        if s in terminal_states:
            R[s, a_i] = 0  # no penalty in terminal states

# Define a uniform random policy (each action equally likely)
policy = np.ones((num_states, num_actions)) / num_actions

def policy_evaluation(policy, P, R, gamma=1.0, theta=1e-6):
    V = np.zeros(num_states)
    while True:
        delta = 0
        for s in range(num_states):
            v = V[s]
            V[s] = 0
            for a in range(num_actions):
                for s_prime in range(num_states):
                    V[s] += policy[s, a] * P[s, a, s_prime] * (R[s, a] + gamma * V[s_prime])
            delta = max(delta, abs(v - V[s]))
        if delta < theta:
            break
    return V

# Evaluate the policy
V = policy_evaluation(policy, P, R, gamma=1.0)

# Print values in grid format
print("State values under uniform random policy:")
for i in range(grid_size):
    print(["{0:6.2f}".format(v) for v in V[i*grid_size:(i+1)*grid_size]])
