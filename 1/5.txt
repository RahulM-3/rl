import numpy as np

# Define MDP components
states = [0, 1, 2, 3] # 4 states
actions = ['left', 'right'] # 2 possible actions per state

# Transition probabilities and rewards: {state: {action: [(prob, next_state, reward)]}}
transition_probs = {
    0: {
        'left': [(1.0, 0, 0)],
        'right': [(1.0, 1, 0)]
    },
    1: {
        'left': [(1.0, 0, 0)],
        'right': [(1.0, 2, 0)]
    },
    2: {
        'left': [(1.0, 1, 0)],
        'right': [(1.0, 3, 1)] # reward 1 for reaching state 3
    },
    3: {
        'left': [(1.0, 3, 0)],
        'right': [(1.0, 3, 0)] # terminal state, no reward
    }
}

gamma = 0.9 # discount factor
theta = 1e-6 # convergence threshold

# Initialize value function
V = np.zeros(len(states))

def value_iteration():
    global V
    while True:
        delta = 0
        for s in states:
            if s == 3:
                continue # skip terminal state
            v = V[s]
            action_values = []
            for a in actions:
                total = 0
                for (prob, next_state, reward) in transition_probs[s][a]:
                    total += prob * (reward + gamma * V[next_state])
                action_values.append(total)
            V[s] = max(action_values)
            delta = max(delta, abs(v - V[s]))
        if delta < theta:
            break
    return V

def extract_policy(V):
    policy = {}
    for s in states:
        if s == 3:
            policy[s] = 'exit'
            continue
        action_values = {}
        for a in actions:
            total = 0
            for (prob, next_state, reward) in transition_probs[s][a]:
                total += prob * (reward + gamma * V[next_state])
            action_values[a] = total
        best_action = max(action_values, key=action_values.get)
        policy[s] = best_action
    return policy

# Run value iteration
optimal_values = value_iteration()
optimal_policy = extract_policy(optimal_values)

# Print results
print("Optimal State Values:")
for i, v in enumerate(optimal_values):
    print(f"V({i}) = {v:.4f}")

print("\nOptimal Policy:")
for s in optimal_policy:
    print(f"Ï€({s}) = {optimal_policy[s]}")
